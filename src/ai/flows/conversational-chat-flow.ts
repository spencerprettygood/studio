
'use server';
/**
 * @fileOverview A conversational AI flow that can also process unstructured prompts.
 *
 * - conversationalChat - Handles user input, either generating an AI response or processing prompts.
 * - ConversationalChatInput - The input type for the flow.
 * - ConversationalChatOutput - The return type for the flow.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import { processUnstructuredPrompts, type ProcessUnstructuredPromptsOutput } from '@/ai/flows/process-unstructured-prompts';
import { generatePromptTemplate } from '@/ai/flows/generate-prompt-template-flow';
import type { ProcessedPromptData } from '@/lib/types';
import { getPromptsTool } from '@/ai/tools/promptLibraryTool';
import { db } from '@/lib/firebase';
import { collection, addDoc, Timestamp } from 'firebase/firestore';


const ConversationalChatInputSchema = z.object({
  userInput: z.string().describe('The text input from the user.'),
});
export type ConversationalChatInput = z.infer<typeof ConversationalChatInputSchema>;

// The output schema defines what the AI can decide to do.
const ConversationalChatOutputSchema = z.object({
  aiResponse: z.string().describe('The AI-generated response to the user input.'),
  action: z.enum(['confirm_save', 'list_prompts', 'generate_prompt_template_form', 'confirm_prompt_template_generation']).optional().describe("The action the AI suggests should be taken based on the conversation."),
  promptToSaveData: z.object({
      name: z.string(),
      template: z.string(),
      description: z.string().optional(),
      category: z.string().optional(),
      tags: z.array(z.string()).optional(),
  }).optional().describe("If action is 'confirm_save', this object contains the data for the prompt to be saved to Firestore."),
});
export type ConversationalChatOutput = z.infer<typeof ConversationalChatOutputSchema>;


// Thresholds for detecting if input is a batch of prompts
const PROMPT_BATCH_MIN_LENGTH = 200; 
const PROMPT_BATCH_MIN_NEWLINES = 1; 

function formatProcessedPromptsForChat(processedPrompts: ProcessedPromptData[]): string {
  if (!processedPrompts || processedPrompts.length === 0) {
    return "I tried to process the text you sent, but I couldn't identify any distinct prompts. You can try rephrasing or adding more details. Or, ask me to create a new prompt!";
  }

  let response = `I've analyzed your text and found ${processedPrompts.length} potential prompt(s):\n\n`;
  processedPrompts.forEach((prompt, index) => {
    response += `üìù **Prompt ${index + 1}: ${prompt.generatedTitle}**\n`;
    response += `   Identified Text: "${prompt.identifiedPrompt}"\n`;
    response += `   Description: ${prompt.description}\n`;
    response += `   Category: ${prompt.suggestedCategory}\n`;
    response += `   Tags: ${prompt.suggestedTags.join(', ')}\n`;
    response += `   üí° Quality & Suggestions: ${prompt.qualityAnalysis}\n`;
    if (prompt.followUpPromptSuggestion) {
      response += `   üîó Follow-up Idea: ${prompt.followUpPromptSuggestion}\n`;
    }
    response += `\n`;
  });
  response += "Let me know if you'd like to save any of these or refine them further!";
  return response;
}

export async function conversationalChat(input: ConversationalChatInput): Promise<ConversationalChatOutput> {
  const containsNewlines = (input.userInput.match(/\n/g) || []).length >= PROMPT_BATCH_MIN_NEWLINES;

  if (input.userInput.length > PROMPT_BATCH_MIN_LENGTH && containsNewlines) {
    try {
      const processingResult: ProcessUnstructuredPromptsOutput = await processUnstructuredPrompts({ unstructuredPrompts: input.userInput });
      const formattedResponse = formatProcessedPromptsForChat(processingResult.processedPrompts);
      return { aiResponse: formattedResponse };
    } catch (error) {
      console.error("Error calling processUnstructuredPrompts flow from conversationalChat:", error);
      if (error instanceof Error && error.message.includes('503')) {
        return { aiResponse: "roFl's AI brain (Gemini) is a bit busy right now. Please try again in a moment!" };
      }
      return { aiResponse: "I encountered an error while trying to process your prompts. Please try again, or ask me something else." };
    }
  } else {
    try {
        const result = await conversationalChatFlow(input);
        
        // If the AI decided to save a prompt, we now write it to Firestore.
        if (result.action === 'confirm_save' && result.promptToSaveData) {
            const promptData = result.promptToSaveData;
            await addDoc(collection(db, "prompts"), {
              name: promptData.name,
              description: promptData.description || `Generated by roFl on ${new Date().toLocaleDateString()}`,
              template: promptData.template,
              tags: promptData.tags || [],
              category: promptData.category || 'Uncategorized',
              createdAt: Timestamp.now(),
              updatedAt: Timestamp.now(),
            });
            
            const finalResponse = {
                ...result,
                aiResponse: `Okay, I've saved "${promptData.name}" to your library. You can view it on the dashboard or ask me to list your prompts.`,
            };
            return finalResponse;
        }

        return result;
    } catch (error) {
        console.error("Error in conversationalChatFlow or subsequent processing:", error);
        if (error instanceof Error && error.message.includes('503')) {
          return { aiResponse: "roFl's AI brain (Gemini) is a bit busy right now. Please try again in a moment!" };
        }
        return { aiResponse: "I'm having a little trouble thinking right now. Please try again in a moment." };
    }
  }
}

const llmPrompt = ai.definePrompt({
  name: 'conversationalChatPrompt',
  input: {schema: ConversationalChatInputSchema},
  output: {schema: ConversationalChatOutputSchema},
  tools: [getPromptsTool, generatePromptTemplate],
  prompt: `You are roFl, a witty, highly intelligent, and slightly irreverent AI assistant for managing and optimizing LLM prompts. Your primary goal is to help users create, refine, organize, and find prompts.

  Conversation Style:
  - Be friendly, engaging, and concise.
  - Use your "roFl" persona: intelligent, a bit cheeky, but always helpful.
  - Keep responses focused on the user's prompt-related tasks.

  Core Capabilities:
  1.  **Conversational Interaction:** Engage in general conversation about prompts.
  2.  **Prompt Generation:**
      *   If the user expresses a need for a new prompt (e.g., "help me make a prompt to write emails," "I need a prompt for X"), initiate a process to help them.
      *   Ask clarifying questions to understand: The *purpose*, key *inputs* or {{variables}}, a description of the *desired output*, and any other additionalContext.
      *   Once you have sufficient details, call the 'generatePromptTemplate' tool.
      *   Present the generated template, suggested name, and explanation clearly to the user.
      *   Example: "Alright, based on what you told me, here's a draft: [template] - I've called it '[suggestedName]'. It's designed to [explanation]. What do you think? Shall I save it?"
  3.  **Saving Prompts:**
      *   If the user asks to save a prompt (one you just generated, one they typed, or one from a processed batch):
          *   Confirm the prompt template text and a name for it.
          *   If not already clear, determine a brief description, a category (e.g., "Writing", "Marketing", "Code"), and a few comma-separated tags.
          *   Then, set the 'action' field to 'confirm_save' and populate the 'promptToSaveData' object with all these details (name, template, description, tags, category).
          *   Your 'aiResponse' string should be a simple confirmation question, e.g., "Got it. I'm ready to save '[Prompt Name]' under the '[Category]' category. Sound good?". The final confirmation message will be handled by the system after the save.
  4.  **Listing/Suggesting Prompts:**
      *   If the user asks to see their prompts (e.g., "show my prompts," "list marketing prompts"), use the 'getPromptsTool'.
      *   Format the list of prompts nicely in your 'aiResponse'.
      *   **Proactive Suggestions:** If the user describes a task, consider using 'getPromptsTool' to find relevant existing prompts and suggest them.

  User's current input:
  {{{userInput}}}
  `,
});

const conversationalChatFlow = ai.defineFlow(
  {
    name: 'conversationalChatFlowInternal',
    inputSchema: ConversationalChatInputSchema,
    outputSchema: ConversationalChatOutputSchema,
  },
  async (input) => {
    try {
      const {output} = await llmPrompt(input);
      if (!output) {
          return { aiResponse: "Hmm, my circuits are a bit tangled right now. Try that again?" };
      }
      return output;
    } catch (error) {
      console.error("Error in conversationalChatFlowInternal (llmPrompt call):", error);
      if (error instanceof Error && (error.message.includes('503') || error.message.toLowerCase().includes('service unavailable') || error.message.toLowerCase().includes('model is overloaded'))) {
        return { aiResponse: "roFl's AI brain (Gemini) seems to be overwhelmed or taking a quick nap. Please try your request again in a moment!" };
      }
      return { aiResponse: "I hit a snag trying to process that. Could you try rephrasing or asking again in a bit?" };
    }
  }
);
